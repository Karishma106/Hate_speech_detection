{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA2rrSep2bGS",
        "outputId": "42c382fc-c817-42e7-e174-c3f77a01dbc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ],
      "metadata": {
        "id": "S1DuZseh339f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data (stopwords and tokenizer)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxaMrJCU4DKp",
        "outputId": "e7ba19a6-2a56-4fb5-eedc-2f732104a4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Dataset/Twitter_Data.csv')"
      ],
      "metadata": {
        "id": "b2nqJJwx4Pih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Remove missing values (NaN)\n",
        "data = data[data['clean_text'].notna()]\n",
        "\n",
        "# Convert non-string values to strings\n",
        "data['clean_text'] = data['clean_text'].apply(lambda x: str(x))"
      ],
      "metadata": {
        "id": "X5QJJbxj5iU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a DataFrame named 'data'\n",
        "import pandas as pd\n",
        "\n",
        "# Remove rows with missing values from the entire DataFrame\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# To remove missing values from a specific column, for example 'label'\n",
        "data.dropna(subset=['clean_text'], inplace=True)\n"
      ],
      "metadata": {
        "id": "U-MZ3lQr4b9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your dataset has 'text' and 'label' columns\n",
        "text_data = data['clean_text']\n",
        "labels = data['category']"
      ],
      "metadata": {
        "id": "JfkswHMz9QD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and punctuation\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply the preprocessing function to the text data\n",
        "text_data = text_data.apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "zkYIe2Ac9TX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "I8seww8V7gx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TF-IDF vectorizer with a maximum of 5000 features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit and transform the preprocessed text data to create the TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n"
      ],
      "metadata": {
        "id": "RQ9xeIDG7TtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "dz0JvFgsGWfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "BHx4WAZBGWRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an SVM classifier (you can tune hyperparameters)\n",
        "classifier = SVC(kernel='linear')\n",
        "classifier.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "a9Dks3kaGaiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "predictions = classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "K6K4h11QGeCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "report = classification_report(y_test, predictions)"
      ],
      "metadata": {
        "id": "MfNq2DdDGf8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H3wI7E1GhdZ",
        "outputId": "6d26c54a-ad31-4cca-8014-2a45a262592b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9028655580781739\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.88      0.82      0.85      7152\n",
            "         0.0       0.88      0.98      0.93     11067\n",
            "         1.0       0.93      0.89      0.91     14375\n",
            "\n",
            "    accuracy                           0.90     32594\n",
            "   macro avg       0.90      0.89      0.89     32594\n",
            "weighted avg       0.90      0.90      0.90     32594\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New tweets you want to classify\n",
        "new_tweets = [\"Just had the worst customer service experience at [Company Name]. They need to work on their support.\", \"answer who among these the most powerful world leader today trump putin modi may \", \"through our vote ensure govt need and deserve anupam kher responds modis appeal for the 2019 elections \"]"
      ],
      "metadata": {
        "id": "0-k0ljbsGs4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and TF-IDF vectorize the new tweets\n",
        "new_tweets = [preprocess_text(tweet) for tweet in new_tweets]\n",
        "new_tweet_features = tfidf_vectorizer.transform(new_tweets)"
      ],
      "metadata": {
        "id": "7ZIPp6ueGufO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = classifier.predict(new_tweet_features)"
      ],
      "metadata": {
        "id": "3UacUmy2Gx8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet, prediction in zip(new_tweets, predictions):\n",
        "    if prediction == -1:\n",
        "        label = \"Hate\"\n",
        "    elif prediction == 0:\n",
        "        label = \"Neutral\"\n",
        "    else:\n",
        "        label = \"Non-Hate\"\n",
        "    print(f'Tweet: \"{tweet}\", Prediction: {label}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8UpYnkBGyyP",
        "outputId": "1d69fa90-b6e9-4330-a380-4366438601be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: \"worst customer service experience company name need work support\", Prediction: Hate\n",
            "Tweet: \"answer among powerful world leader today trump putin modi may\", Prediction: Non-Hate\n",
            "Tweet: \"vote ensure govt need deserve anupam kher responds modis appeal 2019 elections\", Prediction: Neutral\n"
          ]
        }
      ]
    }
  ]
}